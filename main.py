import streamlit as st
import yaml
from langchain.chains import create_history_aware_retriever,create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langgraph.graph import StateGraph
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from typing import Annotated
from langchain_core.tools import tool
from langchain_core.messages import ToolMessage
from conversation_management import ConversationManagement
from model_setup import ModelSetup
from document_manager import DocumentManager 
from langchain_openai import ChatOpenAI
from langchain.schema import (
    HumanMessage,
    AIMessage
)


#Streamlitã®ãƒšãƒ¼ã‚¸è¨­å®šã‚„ãƒ˜ãƒƒãƒ€ãƒ¼ã‚„ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¡¨ç¤ºã€‚
st.set_page_config(
page_title="æ ¼é—˜ã‚²ãƒ¼ãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å­«å­AI",
page_icon="ğŸ¤–"
)
st.header("æ ¼é—˜ã‚²ãƒ¼ãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å­«å­AI ğŸ¤–")
st.sidebar.title("Options")



class State (TypedDict):
    messages: Annotated[list,add_messages]


@tool
def suntzu_ai_tool(question):
    """æ‚©ã‚“ã§ã„ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è§£æ±ºã™ã‚‹å­«å­API"""
    vectorstore = st.session_state.vectorstore
    answer = create_suntzu_qa_chain(question,vectorstore)
    return answer



# æŒ‡å®šã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ç”¨ã„ã¦è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã™ã‚‹é–¢æ•°
def create_suntzu_qa_chain(question,vectorstore):
    try:
        # sessionã‹ã‚‰ä¼šè©±å±¥æ­´ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
        memory = st.session_state.memory
        qa_prompt = st.session_state.qa_prompt
        comparison_prompt = st.session_state.comparison_prompt    
        messages = memory.load_memory_variables({})

        # mmrã‚’ä½¿ç”¨ã—ã€æœ€ã‚‚å¤šæ§˜æ€§ã§é–¢é€£æ€§ã®é«˜ã„3ã¤ã®çµæœã‚’è¿”ã™ã‚ˆã†ã«è¨­å®šã€‚
        retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k":3}
        )

        # ä¼šè©±å±¥æ­´ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½¿ã£ãŸretrieverã‚’ä½œæˆ
        suntzu_answer_retriever = create_history_aware_retriever(llm,retriever,qa_prompt)
        # æ–‡æ›¸ã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
        comparison_answer_chain = create_stuff_documents_chain(llm,comparison_prompt)
        # æ¤œç´¢ã¨å›ç­”ç”Ÿæˆã‚’çµ±åˆã—ãŸretrieval_chainã‚’ä½œæˆ
        chain = create_retrieval_chain(suntzu_answer_retriever, comparison_answer_chain)
        # chainã‚’å®Ÿè¡Œã—ã€è³ªå•ã¨ä¼šè©±å±¥æ­´ã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆ
        result = chain.invoke({"input": question, "chat_history": messages["history"]})

        return result["answer"]
    
    except Exception as e:
        st.error("å›ç­”ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        return None



#ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹é–¢æ•°
def chatbot(state):
    state["messages"].append(llm_with_tool.invoke(state["messages"]))
    return state

# ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®é–¢æ•°
def tool(state):
    try:
        tool_by_name = {"suntzu_ai_tool":suntzu_ai_tool}
        last_message = state["messages"][-1]

        # æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°æƒ…å ±ã‚’ä½¿ã£ã¦ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™
        tool_function = tool_by_name[last_message.tool_calls[0]["name"]]
        # ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’å–å¾—
        tool_output = tool_function.invoke(last_message.tool_calls[0]["args"])
        state["messages"].append(ToolMessage(content = tool_output ,tool_call_id = last_message.tool_calls[0]["id"]))

        return state
    except Exception as e:
        st.error("ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        return None


# ã‚°ãƒ©ãƒ•æ§‹é€ ã®ä½œæˆã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
graph = StateGraph(State)

# ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
graph.add_node("chatbot",chatbot)
graph.add_node("tool",tool)

# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ãƒ•ã‚£ãƒ‹ãƒƒã‚·ãƒ¥ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š
graph.set_entry_point("chatbot")
graph.set_finish_point("tool")

# ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
graph.add_edge("chatbot","tool")

runner = graph.compile()




# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿œã˜ã¦å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_response(question,runner):
    with st.spinner("ChatGPT is typing ..."):
        response = runner.invoke({"messages":[question]})
    return response["messages"][-1].content


#ã€€ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
cm = ConversationManagement()
ms = ModelSetup()
dm = DocumentManager()



#ä¼šè©±å±¥æ­´ã‚’å‡¦ç†ã™ã‚‹
cm.init_messages()

#ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
container = st.container()

#ãƒ¢ãƒ‡ãƒ«ã¨ãƒ©ãƒ³ã‚¯ã®é¸æŠ
llm = ChatOpenAI(model_name ="gpt-4o-mini-2024-07-18")
rank = ms.select_rank()

# LLMã«ãƒ„ãƒ¼ãƒ«ã‚’è¨­å®š
llm_with_tool = llm.bind_tools([suntzu_ai_tool])


# YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ 
if"pdf_files" not in st.session_state:
    with open("config.yaml","r") as file: 
        config = yaml.safe_load(file)

    # pdf_filesãƒªã‚¹ãƒˆã‚’å–å¾— 
    st.session_state.pdf_files = config["pdf_files"]

# PDFã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰²ã‚’è¡Œã†
if "split_sections" not in st.session_state :  
    pdf_files = st.session_state.pdf_files
    st.session_state.split_sections = dm.load_and_process_pdfs(pdf_files)
    

# åˆ†å‰²ã—ãŸå†…å®¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹
if "vectorstore" not in st.session_state:
    split_sections = st.session_state.split_sections
    st.session_state.vectorstore = dm.create_vectorstore(split_sections)


if question := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼"):
    # ãƒ©ãƒ³ã‚¯ã”ã¨ã«å¯¾å¿œã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ 
    st.session_state.qa_prompt = cm.generate_prompt(rank)
    st.session_state.comparison_prompt = cm.generate_comparison_prompt(rank)
    results = get_response(question,runner)
else:
    results = None


if results:
    memory = st.session_state.memory
    memory.save_context({"input": question}, {"output": results})
    messages = memory.load_memory_variables({})
    with container:
        for result in messages["history"]:
            if isinstance(result, AIMessage):
                with st.chat_message("assistant"):
                    st.markdown(result.content)
            elif isinstance(result, HumanMessage):
                with st.chat_message("user"):
                    st.markdown(result.content)


